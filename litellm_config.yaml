# litellm_config.yaml

model_list:
  # 1. MAF Default Alias (Maps to local Ollama service)
  - model_name: "maf-default"
    litellm_params:
      # LiteLLM routing to the Ollama service running in the Docker network
      model: "ollama_chat/llama3.1"
      api_base: "http://maf-ollama:11434"
    model_info:
      supports_function_calling: true

  # 2. Local Ollama Model (Matches the OLLAMA_MODEL_NAME in settings.py)
  - model_name: "maf-ollama/llama3.1"
    litellm_params:
      model: "ollama_chat/llama3.1"
      api_base: "http://maf-ollama:11434"
    model_info:
      supports_function_calling: true

  # 3. Gemini Cloud Model (Matches the GEMINI_MODEL_NAME in settings.py)
  - model_name: "gemini-2.5-flash"          # ðŸ‘ˆ Client sends this (The short name)
    litellm_params:
      # CRITICAL FIX: LiteLLM requires the 'gemini/' prefix for internal routing
      model: "gemini/gemini-2.5-flash"    # ðŸ‘ˆ LiteLLM maps the short name to the provider's format
    model_info:
      supports_function_calling: true

# CRITICAL FIX: API Keys Registration (Proxy Authentication)
# This section tells the LiteLLM proxy which security tokens to accept from the client.
api_keys:
  # The key sent by the CoreAgent client must be registered here.
  # This key must match the LITELLM_MASTER_KEY used by your client code.
  "sk-maf-secure-2025-key": "local-agent-key"
